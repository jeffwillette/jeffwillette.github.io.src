---
title: Logistic Regression
published: true
createdAt: 2019-02-15T12:38:13.957Z
updatedAt: 2019-02-28T11:34:06.531Z
images:
  - ./sigmoid.svg
categories:
  - Programming
  - Algorithms
  - Machine Learning
---

## Logistic Regression

<Image1 align="left" width="40%" />

Linear regression is good for predicting values that have a linear relationship to the data. In classification
problems, however, there are a limited number of values that are being predicted. Like "Is this a
picture of a dog?", "Is this a spam email?", etc.

The hypothesis function is then given in terms of the likelihood of `x` being classfied as `0` or `1`. The `g(z)`
below of the function that produces the curve in the picture.

<BlockMath math="
  \begin{aligned}
   &h_\theta(x) = g(\theta^Tx) \\
   &z = \theta^Tx \\
   &g(z) = \frac{1}{1 + e^{-z}}
  \end{aligned}
"/>


If we are only choosing from two possibilities, then the following must be true...

<BlockMath math="h_\theta(x) = P(y = 1 | x;\theta) = 1 - P(y = 0|x;\theta)" />

## Decision Boundary

From the equations and chart above we can deduce that when <InlineMath math="z = 0" />
then <InlineMath math="g(z) = 1/2" />. So if `z` where to be more than 0, teh result would then be greater than `0.5`.
Since <InlineMath math="\theta^Tx" /> can be substituted for <InlineMath math="z" /> we can then say the following.

<BlockMath math="
 \begin{aligned}
   &g(z) >= 0.5 \quad when \quad z \geq  0 \\
   &h_\theta(x) = g(\theta^Tx) \geq 0.5 \quad whenever \quad \theta^Tx \geq 0 \\
 \end{aligned}
" />

The same is true in the opposite case, whenever <InlineMath math="\theta^Tx" /> is less than 0, then `g(z)` will be less
than 0.5.

## The Cost Function and Gradient Descent

For logistic regression, the cost function is also the mean of the sum of all the costs, but instead of the mean of
squared error, it is defined as...

<BlockMath math="
  \begin{aligned}
    J(\theta) &= \frac{1}{m} \sum_{i=1}^m Cost(h_\theta(x^{(i)}), y^{(i)}) \\
    Cost(h_\theta(x), y) &= \begin{cases}
     -log(h_\theta(x)) &\text{if } y = 1 \\
     -log(1 - h_\theta(x)) &\text{if } if y = 0
    \end{cases} \\
    Cost(h_\theta(x), y) &= -y * log(h_\theta(x)) - (1 - y)log(1 - h_\theta(x)) \\
    J(\theta) &= \frac{1}{m} (-y^T log(h_\theta(x))) - (1 - y)^T log(1 - h_\theta(x))))
  \end{aligned}
" />

In order to run gradient descent with this cost function we need to take the partial derivative of the cost function and
update each theta just like the linear regression method.

<BlockMath math="
  \begin{aligned}
    \theta_j &:= \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta) \\
    \theta_j &:= \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_J^{(i)} \\
    \theta &:= \theta - \frac{\alpha}{m} X^T(g(X\theta) - \overrightarrow{y})
  \end{aligned}
" />
