---
title: Matrices and Gaussian Elimination
published: true
createdAt: 2019-07-05T01:16:00.496Z
updatedAt: 2019-07-09T04:13:23.027Z
categories:
  - Math
  - Linear Algebra
---

import { M } from '../../../src/components/math.tsx';

## Preface and 1.2 The Geometry of Linear Equations

- the **column space** all combinations of the columns
- the **row space** all combinations of the rows
- the **rank** the number of independent columns or rows
- **elimination** is the good way to find the rank of a matrix

Independence of columns is defiend as the the columns or rows cannot be defined as a linear combination of the other
rows or columns. If they cannot be defined as a combination of the other rows and columns then the column or row is said
to be independent.

A series of linear equations may not have a solution but it always has a best squares solution (regression).

<M i="LU" /> decomposition factorizes a matrix into a lower triangular and upper triangular factors. **upper
triangular matrix** has numbers in the upper part of the triangle and `0`'s in the lower part. **lower triangular matrix**
has numbers in the lower part and `0`'s in the upper part.

during elimination, there is no solution when a set of linear equations is parallel or if they are on the same line. If
you try to run gaussian elimination on the equations...

<M b="1x + 2y = 3 \\ 4x + 8y = 6" />

you will see that you end up with the result <M i="0 = -6" />. If the equation was changed to be the smae line with
different coefficients, then the line would be parallel and have infinitely many solutions.

<M b="1x + 2y = 3 \\ 4x + 8y = 12"/>

The above equation would yield the result <M i="0 = 0" /> because they are the same line.

- in three dimensions, a line required two equations. In `n` dimensions, a line will require `n - 1` equations.

Linear equations can have `n-dimensions`. To visualize easier, I like to think of 3 dimensional space. The first
equation produces an `n-1` dimensional plane (2 dimensions). The second equation will produce a plane that intersects
the first plane which will be an `n-2` dimensional plane (1 dimensional line). The final plane will intersect that at a
single point. Each equation cuts the number of dimensions by one until there is a point with zero dimensions.

A **linear combination** is a combination of vectors that have been multiplied by a scalar value that are added
together.

When looking at a row picture, there are `n` planes, a column picture has `n` vectors. We are trying to find linear
combinations of the n columns that equals the right side of the equation <M i="b" />.

An **invertible** matrix is also known as a **singular** matrix where <M i="AB = BA = I" /> where <M i="I" /> is the
identity matrix.

A matrix **determinant** is a scalar property of the matrix which can be thought of a the physical volume enclosed by
the matrix.

If the columns vectors of an `n` dimensional space are independent, then they will fill out the entire n dimensional
space. If they are not independent, theb they will fill out the `n - (non independent)` dimensional space. If there are
9 dimensions and only 8 of the columns are independent, then the columns will fill out the entire 8 dimensional plane
within 9 dimensional space.

<M b="
\begin{aligned}
  \begin{bmatrix}
    2 & 5 \\
    1 & 3
  \end{bmatrix}
  \begin{bmatrix}
    1 \\
    2
  \end{bmatrix}
  = 1 \begin{bmatrix}
    2 \\
    1
  \end{bmatrix} + 2
  \begin{bmatrix}
    5 \\
    3
  \end{bmatrix} =
  \begin{bmatrix}
    12 \\ 7
  \end{bmatrix}
\end{aligned}
"/>

If `n` planes have no point in common, or infinitely many points, then the n columns lie in the same plane.

The new part of this lecture to me was the column picture. It is much less intuitive to me to look at it this way. The
solution is a combination of the column vectors where the outcome is the right hand side of the equation. This
particular combination of the column vectors will give the solution of `x` and `y`.

## 1.3 Gaussian Elimination

The goal of Gaussian elimination is to subtract multiples of each equation from the subsequent rows until there is an
upper triangular matrix. The bottom row will then be a single value of a dimension, and it can be back substituted row
by row to find the solution to the system.

The current column that is being solved for is called the pivot. If there is a 0 in a pivot place as the algorithm
progresses, equations can be swapped to move a usable pivot into the current position. If there is nothing that can be
swapped to make a non-zero pivot, then there is either no solution or infinitely many solutions.

There will be no solution when remaining equations are inconsistent as in the following matrix.

<M b="
\begin{bmatrix}
  1 & 1 & 1 \\
  0 & 0 & 3 \\
  0 & 0 & 4 \\
\end{bmatrix} =
\begin{bmatrix}
  b_1 \\ 6 \\ 7
\end{bmatrix}
"/>

This is becuase `3w = 6` and `4w = 7` have no compatible solution and therefore the whole set has no solution. If, in
fact the last two rows of the matrix were `3w = 6` and `4w = 8` then there would be infinitely many solutions since
those rows are not independent and therefore there are infinitely many solutions along the one remaining dimension that
is not independent.

## Permutation Matrices

matrices can be multiplied by other matrices that permute the rows as show below.

<M b="
P_{23} =
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 0 & 1 \\
  0 & 1 & 0
\end{bmatrix}
" />

What this matrix is doing is taking one of row one in the first line, taking one of row three in the second line, and
taking one of row two in the third line which swaps column 2 and 3. Permutation matrices can be multiplied together if
in the right order to make their permutations happen simultaneously.

## LU Decomposition

LU decomposition happens by the following formula <M i="PA = LU"/> where <m i="PA"/> is the permutation matrix to make
sure that there will be no 0's in the pivots. <M i="L" is the lower trinagular matrix that stores how many of which
columns were subtracted from the matrix to arrive at <M i="U" /> which is the upper trinagular matrix and can be used to
solve the set of linear equations.

## Inverses

- if you multiply something by <M i="A" /> and then by <M i="A^{-1}" /> then you are back to the starting point. <M
  i="\text{if } b = Ax \text{ then } A^{-1}b = x"/>.
- <M i="AA^{-1} = I" />
- multiplication order does not matter in inverses <M i="AB = I \text{ is also the same as } BA = I" />.
- the inverse exists only when elimination produces n pivots.
- a mtrix cannot have more than one inverse.
- if <M i="Ax = b" /> then the solution is <M i="x = A^{-1}b" />.
- if x is a nonzero vector and <M i="Ax = 0" /> then <M i="A" /> is not invertible. Nothing can be multiplied by 0 to
  get to <M i="x"/>.
- a matrix is invertible only if its determinant is nonzero.
- a diagnoal matrix has an inverse as long as no entries are zero.
- when solving matrix equation algebraically, the inverse matrices go in the reverse order on the other side of the
  equation. <M i="ABx = y" /> is the same as <M i="x = B^{-1}A^{-1}y" />.
- if there is a 0 vector in the matrix then the matrix is not invertible because nothing can take

The Gauss-Jordan method of finding the inverse is to setup a matrix with the identity matrix as the augmented right side
part and then run row reduce on all of the rows, until there is an identity matrix on the left and the inverse is on the
right.

<M b="
\begin{bmatrix}
  2 & 1 & 1 & 1 & 0 & 0 \\
  4 & -6 & 0 & 0 & 1 & 0 \\
  -2 & 7 & 2 & 0 & 0 & 1 \\
\end{bmatrix} \implies
\begin{bmatrix}
  1 & 0 & 0 & \frac{12}{16} & -\frac{5}{16} & -\frac{6}{16} \\
  0 & 1 & 0 & \frac{4}{8} & -\frac{3}{8} & -\frac{2}{8} \\
  0 & 0 & 1 & -1 & 1 & 1 \\
\end{bmatrix}
"/>

## Transposes

- <M i="AB^T = B^T A^T" />.
- <M i="(A^{-1})^T = (A^T)^{-1}" />.

## Symmetric Matrices

- the diagonal can be anything as long as the lower and upper triangular parts are mirror images of each other.
- if the matrix is invertible, then <M i="A^{-1}" /> is also invertible.
- any matrix multiplied by its transposed self is symmetric. <M i="AA^T" /> is a symmetric square matrix. <M i="A^TA" />
  is also a symmetric square matrix but it is different.
